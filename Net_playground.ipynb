{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mimg\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageColor, ImageFont, ImageDraw, ImageFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import DL stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import tensorflow as tf\n",
    "\n",
    "import skimage.io as io\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helpers import Converter, Resize, Normalize, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf: 1.12.0 \n",
      "\r",
      " torch: 1.0.0\n"
     ]
    }
   ],
   "source": [
    "print('tf:', tf.__version__, '\\n\\r', 'torch:', torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataStream(Dataset):\n",
    "    def __init__(self, fname, transform=None):\n",
    "        self.data = pd.read_csv(fname, sep=';', header=None)\n",
    "        self.imgs = self.data[0]\n",
    "        self.labels = self.data[1]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = io.imread(self.imgs[idx])\n",
    "        y = self.labels[idx]\n",
    "        sample = {'img' : x, 'label' : y}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resize(object):\n",
    "    \"\"\"Resize.\"\"\"\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        img = cv2.resize(sample['img'], self.size)\n",
    "        return {'img' : img, 'label' : sample['label']}\n",
    "\n",
    "class ToTensorTarget(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        sat_img, label = sample['img'], sample['label']\n",
    "        return {'img': transforms.functional.to_tensor(sat_img.copy()),\n",
    "                'label' : sample['label']}\n",
    "    \n",
    "class NormalizeTarget(transforms.Normalize):\n",
    "    \"\"\"Normalize a tensor and also return the target\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        # print(sample)\n",
    "        return {'img': transforms.functional.normalize(sample['img'], self.mean, self.std),\n",
    "                'label': sample['label']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([Resize((204, 32)),\n",
    "                               ToTensorTarget(),\n",
    "                               NormalizeTarget([0.3956, 0.5763, 0.5616],\n",
    "                                                [0.1535, 0.1278, 0.1299])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DataStream('./data/data.csv', transform=transform)\n",
    "ds = DataLoader(dr, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import crnn\n",
    "from configs import generator_cfg\n",
    "from helpers import Converter\n",
    "\n",
    "gen_cfg = generator_cfg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = Converter(gen_cfg.alph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = crnn.CRNN(3, len(gen_cfg.alph) + 1, 256).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = V(sample['img'].to(device))\n",
    "Y_labels, Y_lengths = converter.encode(sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 512, 1, 52])\n",
      "torch.Size([52, 100, 71])\n"
     ]
    }
   ],
   "source": [
    "y_hat = model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CTCLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gen_cfg.alph) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_size = torch.IntTensor(100).fill_(y_hat.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = criterion(y_hat, Y_labels, preds_size, Y_lengths) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0e0e0e0e0',\n",
       " 'esese0',\n",
       " 'e0Re0R0Rse0',\n",
       " '0e0ese0e',\n",
       " 'e0e0ese0',\n",
       " 'e0e0b0e0',\n",
       " 'edesesese',\n",
       " 'e0seRed0eReR0R0R0',\n",
       " '0e0e0',\n",
       " 'eReReRe',\n",
       " 'ede0e0ese0',\n",
       " '0edsede',\n",
       " 'e0s0e0s0',\n",
       " '0e0e0es0e',\n",
       " '0ReR',\n",
       " 'e0eReje0',\n",
       " 'eReReRe0',\n",
       " 'e0e0e0e0e0',\n",
       " '0e0R0e0ReRQe0e',\n",
       " '0eReRe0e0',\n",
       " '0eQe0de0e',\n",
       " 'e0e0e0',\n",
       " '0eRd.ede0',\n",
       " 'e0',\n",
       " '0e0eRQReRe0',\n",
       " 'e0e0ede0',\n",
       " '0eRe0',\n",
       " '0eResR0ese0',\n",
       " 'eReRe0e0',\n",
       " 'eReRe0e0',\n",
       " 'eReReRe0',\n",
       " 'eReReReRe0ese0',\n",
       " 'e0e0e0',\n",
       " 'es0ese0',\n",
       " 'e0e0Re0',\n",
       " 'ese0',\n",
       " 'e',\n",
       " 'ese0',\n",
       " 'edese0',\n",
       " '0d0e0e0ede',\n",
       " 'e0Rese0',\n",
       " 'eRese0',\n",
       " '0e0e0de0e0',\n",
       " 'e0es0se0',\n",
       " '0e0e0e',\n",
       " 'e0e0e0e0e0',\n",
       " 'eReReRese0',\n",
       " 'eReReR0R',\n",
       " '0ReRe',\n",
       " 'eReRe0',\n",
       " 'eReR0e0e',\n",
       " '0e0eRe',\n",
       " 'eReReReRe0',\n",
       " '0e0e0ese0',\n",
       " 'eResResR0Re0',\n",
       " 'eReRsRese0',\n",
       " 'e0e0eRe',\n",
       " 'es0ese0',\n",
       " 'e0',\n",
       " 'Re0',\n",
       " 'eR0e',\n",
       " 'ese0',\n",
       " 'jRe0',\n",
       " 'eRes0ese0',\n",
       " 'R0R0R0',\n",
       " 'e0ese0',\n",
       " 'ebsbsesbsbese0',\n",
       " '0e',\n",
       " '0Re0esR0ReRe0',\n",
       " 'e0de',\n",
       " 'ese0bdse0e0Rdse0',\n",
       " '0e0esesRe0eRe',\n",
       " 'seRQ0e',\n",
       " 'eReRe',\n",
       " 'ese0',\n",
       " 'edsese0s0e0',\n",
       " 'eReR0R0Rese0',\n",
       " 'eRe',\n",
       " '0eReReReR',\n",
       " 'j0eReR',\n",
       " 'eReR',\n",
       " '0esese0',\n",
       " 'e0dese0',\n",
       " '0e0e0R0e0e',\n",
       " '0e0ese0',\n",
       " 'e0e0ReRese0',\n",
       " 'ese0',\n",
       " 'eResejRe0',\n",
       " 'eR0eRes0eReRe0',\n",
       " 'eRe0',\n",
       " 'eR0eseRdese0',\n",
       " 'eRe',\n",
       " 'esesese0',\n",
       " 'eReR',\n",
       " 'e0Re0e0Rese0',\n",
       " '0ReReR',\n",
       " '0ReResese0',\n",
       " 'eReReR',\n",
       " 'e0edesese0',\n",
       " '0edese0']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.best_path_decode(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([10, 11,  7, 60, 59, 53, 58, 64, 13,  7, 22, 18,  7, 60, 59, 53, 58, 64,\n",
       "         12, 62, 48, 19,  8, 30,  8, 45, 45, 56, 63, 19,  8, 24,  8, 19, 13, 19,\n",
       "          8, 30,  8, 34,  8, 14, 64, 52, 45, 11, 58, 48, 19,  8, 32,  8, 10,  9,\n",
       "         17,  9, 13, 26, 19,  8, 27,  8, 45, 45, 52, 10, 10,  7, 60, 59, 53, 58,\n",
       "         64, 13, 64, 52, 45, 45, 52, 63, 10,  9,  7, 60, 59, 53, 58, 64, 16,  7,\n",
       "         60, 59, 53, 58, 64, 14,  7, 38, 45, 45, 56, 53, 53, 63, 19, 45, 56, 46,\n",
       "         59, 62, 51, 19, 19, 27, 27, 17,  7, 60, 59, 53, 58, 64, 18, 64, 52, 19,\n",
       "          8, 36,  8, 21,  8, 37,  8, 12, 22, 11,  6, 13,  7, 48, 11, 19,  4, 31,\n",
       "         19, 10, 11,  9,  7, 60, 59, 53, 58, 64, 45,  8, 62,  8, 19,  8, 31,  8,\n",
       "         22,  8, 25,  8, 19, 45, 56, 49, 63, 65, 58, 48, 10, 17,  7, 60, 59, 53,\n",
       "         58, 64, 12, 31,  4, 47, 19,  8, 24,  8, 19,  8, 31,  8, 13, 25, 30, 45,\n",
       "         45, 52, 53, 58, 51, 19, 19, 19, 19, 16, 64, 52, 19,  8, 25,  8, 19, 19,\n",
       "         24, 45,  5, 19, 19, 23, 23, 19,  8, 27,  8, 22,  8, 19,  8, 27,  8, 19,\n",
       "          8, 19, 19, 31, 19,  8, 20,  8, 45,  8, 60,  8, 19, 19, 19, 19,  8, 39,\n",
       "          8, 21,  8, 19, 19, 31, 37, 27, 19,  8, 15,  7, 60, 59, 53, 58, 64, 19,\n",
       "          8, 19,  8, 19,  8, 19, 14, 19,  8, 21,  8, 15, 64, 52, 19, 19, 19, 30,\n",
       "         45,  7, 10, 15,  7, 60, 59, 53, 58, 64, 19, 45, 56, 64, 59, 19,  8, 22,\n",
       "          8, 19, 19, 25, 19,  8, 26,  8, 19,  8, 20,  8, 19,  8, 19,  4, 34, 19,\n",
       "         45, 56, 63, 64, 13, 17,  7, 60, 59, 53, 58, 64, 19,  7, 10, 19, 19, 23,\n",
       "         17, 64, 52, 45, 45, 52, 49, 48, 19,  8, 31,  8, 19,  8, 41,  8, 33,  8,\n",
       "         30,  8, 10,  9, 64, 52, 45, 45, 56, 19,  8, 31,  8, 19,  8, 11,  6, 13,\n",
       "          6, 14,  7, 64, 19, 19, 19, 37, 12,  7, 22, 19,  8, 39,  8, 19, 19, 19,\n",
       "          8, 22,  8, 21,  8, 19, 45, 47, 52, 49, 58, 45, 45, 56, 53, 53, 10, 63,\n",
       "         64, 45,  8, 67,  8, 19,  8, 40,  8, 19, 19, 19, 19, 19, 19, 11, 22, 19,\n",
       "         45, 46, 49, 62, 51, 14,  7, 60, 59, 53, 58, 64, 12,  9,  7, 12,  9],\n",
       "        dtype=torch.int32),\n",
       " tensor([8, 3, 7, 3, 4, 4, 4, 2, 6, 3, 1, 3, 4, 4, 2, 4, 3, 8, 3, 4, 8, 7, 3, 6,\n",
       "         7, 4, 7, 3, 8, 2, 5, 1, 3, 2, 8, 4, 8, 8, 8, 2, 2, 8, 3, 6, 4, 3, 4, 3,\n",
       "         2, 4, 6, 6, 3, 4, 4, 3, 6, 5, 2, 7, 6, 2, 4, 3, 4, 2, 8, 5, 4, 3, 4, 6,\n",
       "         3, 5, 8, 3, 3, 3, 5, 4, 8, 4, 3, 6, 7, 4, 3, 4, 2, 6, 6, 5, 3, 4, 4, 6,\n",
       "         2, 6, 7, 5], dtype=torch.int32))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.encode(sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 1, 71])\n",
      "torch.Size([71, 1, 32])\n"
     ]
    }
   ],
   "source": [
    "H = 32\n",
    "W = 280\n",
    "C = 1\n",
    "net = CRNN(C, nc=32, nh=128)\n",
    "net.apply(weights_init)\n",
    "X = V(torch.randn(1, C, H, W))\n",
    "Y_ = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = nn.CTCLoss("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrDict(dict):\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "    \n",
    "def define_config():\n",
    "    config = AttrDict()\n",
    "    config.n_classes = 34\n",
    "    config.lstm_size = 256\n",
    "    config.width = 280\n",
    "    config.height = 32\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_classes': 34, 'lstm_size': 256}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.rnn import BasicLSTMCell\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "slim = tf.contrib.slim\n",
    "cfg = define_config()\n",
    "\n",
    "def cnn(inputs, scope='vgg', is_training=True):\n",
    "    batch_norm_params = {'is_training': is_training}\n",
    "    with slim.arg_scope([slim.conv2d], \n",
    "                        normalizer_fn=slim.batch_norm, \n",
    "                        normalizer_params=batch_norm_params,):\n",
    "        with slim.arg_scope([slim.batch_norm], **batch_norm_params):\n",
    "            net = slim.repeat(\n",
    "                inputs, 1, \n",
    "                slim.conv2d, \n",
    "                64, [3, 3], \n",
    "                scope='conv1',\n",
    "            )\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool1')\n",
    "            net = slim.repeat(net, 1, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "            net = slim.max_pool2d(net, [2, 2], scope='pool2', padding='SAME')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "            net = slim.max_pool2d(net, [2, 2], stride=[2, 1], scope='pool3', padding='SAME')\n",
    "            net = slim.repeat(net, 2, slim.conv2d, 512, [3, 3], scope='conv4')\n",
    "            net = slim.max_pool2d(net, [2, 2], stride=[2, 1], scope='pool4', padding='SAME')\n",
    "            net = slim.repeat(net, 1, slim.conv2d, 512, [3, 3], scope='conv5')\n",
    "            return net\n",
    "\n",
    "\n",
    "def crnn(images, is_training=True):\n",
    "    dropout_keep_prob = 0.7 if is_training else 1.0\n",
    "    cnn_net = cnn(images, is_training=is_training)\n",
    "    with tf.variable_scope('Reshaping_cnn'):\n",
    "        shape = cnn_net.get_shape().as_list()  # [batch, height, width, features]\n",
    "        transposed = tf.transpose(cnn_net, perm=[0, 2, 1, 3],\n",
    "                                  name='transposed')  # [batch, width, height, features]\n",
    "        conv_reshaped = tf.reshape(transposed, [shape[0], -1, shape[1] * shape[3]],\n",
    "                                   name='reshaped')  # [batch, width, height x features]\n",
    "\n",
    "    list_n_hidden = [cfg.lstm_size, cfg.lstm_size]\n",
    "\n",
    "    with tf.name_scope('deep_bidirectional_lstm'):\n",
    "        # Forward direction cells\n",
    "        fw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "        # Backward direction cells\n",
    "        bw_cell_list = [BasicLSTMCell(nh, forget_bias=1.0) for nh in list_n_hidden]\n",
    "\n",
    "        lstm_net, _, _ = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(fw_cell_list,\n",
    "                                                                        bw_cell_list,\n",
    "                                                                        conv_reshaped,\n",
    "                                                                        dtype=tf.float32\n",
    "                                                                        )\n",
    "        # Dropout layer\n",
    "        lstm_net = tf.nn.dropout(lstm_net, keep_prob=dropout_keep_prob)\n",
    "        #logging.info('after lstm shape: %s' % lstm_net.shape)\n",
    "\n",
    "    with tf.variable_scope('fully_connected'):\n",
    "        shape = lstm_net.get_shape().as_list()  # [batch, width, 2*n_hidden]\n",
    "        fc_out = slim.layers.linear(lstm_net, cfg.n_classes)  # [batch x width, n_class]\n",
    "        #logging.info('fc_out shape: %s' % fc_out.shape)\n",
    "\n",
    "        lstm_out = tf.reshape(fc_out, [shape[0], -1, cfg.n_classes],\n",
    "                              name='lstm_out')  # [batch, width, n_classes]\n",
    "        #logging.info('lstm_out shape: %s' % lstm_out.shape)\n",
    "\n",
    "        # Swap batch and time axis\n",
    "        logprob = tf.transpose(lstm_out, [1, 0, 2], name='transpose_time_major')  # [width(time), batch, n_classes]\n",
    "\n",
    "        return logprob\n",
    "\n",
    "\n",
    "def create_loss(sparse_code_target, logprob, seq_len_inputs):\n",
    "    with tf.control_dependencies(\n",
    "            [tf.less_equal(sparse_code_target.dense_shape[1], tf.reduce_max(tf.cast(seq_len_inputs, tf.int64)))]):\n",
    "        loss_ctc = tf.nn.ctc_loss(labels=sparse_code_target,\n",
    "                                  inputs=logprob,\n",
    "                                  sequence_length=tf.cast(seq_len_inputs, tf.int32),\n",
    "                                  ignore_longer_outputs_than_inputs=True,\n",
    "                                 )\n",
    "        loss_ctc = tf.reduce_mean(loss_ctc)\n",
    "    return loss_ctc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0211 16:57:17.981990 140354946066176 tf_logging.py:161] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fa648325710>: Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU.\n",
      "W0211 16:57:17.984064 140354946066176 tf_logging.py:161] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fa648325940>: Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU.\n",
      "W0211 16:57:17.985587 140354946066176 tf_logging.py:161] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fa648325208>: Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU.\n",
      "W0211 16:57:17.987194 140354946066176 tf_logging.py:161] <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fa6483255c0>: Note that this cell is not optimized for performance. Please use tf.contrib.cudnn_rnn.CudnnLSTM for better performance on GPU.\n"
     ]
    }
   ],
   "source": [
    "ret = crnn(np.random.rand(1,224,64,3).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(16), Dimension(1), Dimension(34)])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text):\n",
    "    if isinstance(text, str):\n",
    "        te"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
